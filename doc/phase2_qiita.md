# 【ComfyUI】基本操作とAPI連携（JSON書き出し）の基礎

低コストで自由度の高いAI動画生成環境を構築するロードマップ（Phase 2: GUI習得とAPI自動化）の前半として、ComfyUIの基本操作とAPI連携の基礎を解説する。
クラウド環境（Paperspace等）のリソース制限下でも快適に動作する「SD1.5」をベースモデルとして採用し、GUIでの画像生成から、プログラム連携（Python等）に必須となる「API Format JSON」の出力手順までを網羅する。

## ロードマップにおける本記事の位置づけ

本記事は、筆者が取り組んでいる「画像生成AI開発ロードマップ」の **第1段階（Phase 1）** にあたる実践編である。  
全体の構想や技術選定の背景については、前回のロードマップ記事を参照してほしい。

https://qiita.com/YukiF0511/items/92ba44b379241de11a96

## 本記事のゴール（DoD）

GUIの操作習得には軽量なSD1.5を採用し、将来的な自動化を見据えて「API Format JSON」によるワークフローの書き出し手法を確立する。
これを踏まえた本記事の完了定義（DoD）は以下の通りである。

- SD1.5モデルを導入し、プロンプトから1枚の画像が正常に生成（T2I）できる仕組みを理解していること
- ComfyUIからAPI用のJSON形式でワークフローを書き出し、外部プログラムから書き換えるべきパラメータのデータ構造を把握していること

## 1. ComfyUIアーキテクチャと基本ノードの役割

ComfyUIはノード（処理ブロック）をワイヤーで繋ぐことで、画像生成のパイプラインを構築する。データがどのように流れ、変換されていくかを理解することが重要である。

### 1.1 UIと基本操作
キャンバス上での基本的なマウス操作は以下の通りである。
- **視点移動（パン）**：キャンバスの空白部分を左クリックしたままドラッグ、またはスペースキー＋左ドラッグ
- **ズーム**：マウスホイールの回転
- **ノードの移動**：ノード上部のヘッダ部分を左ドラッグ
- **ワイヤーの接続**：ノードの丸い端子（ピン）を左ドラッグして別の端子へ繋ぐ
- **ワイヤーの解除**：繋がっている端子上で右クリック

### 1.2 画像生成（Text to Image）のデータフロー
画像生成は「テキスト → ベクトルデータ → 潜在空間でのノイズ除去 → ピクセル画像」というフローで進行する。これらを担う必須ノードの役割は以下の通りである。

- **Load Checkpoint**
  画像生成のコアとなる学習済みAIモデル（.safetensors等）を読み込み、以下の各ノードへ必要な情報を供給する。
  ※本記事では、ファイルサイズが小さく（約2GB程度）動作が軽快で、初期の学習・動作確認に最適な「SD1.5」をベースモデルとして想定する。
- **CLIP Text Encode**
  人間の言葉（テキスト）をAIが理解できる数値データ（ベクトル）に翻訳する
  - ポジティブプロンプト：描かせたい要素（主題、背景、画質など）
  - ネガティブプロンプト：描かせたくない要素（低画質、構造の崩れなど）
- **KSampler（Kサンプラー）**
  画像生成プロセスの心臓部（推論エンジン）であり、完全なノイズ状態から画像を少しずつ削り出す（デノイズ）処理を行う
  - Steps：ノイズを削る回数（通常20〜30）。多いほど緻密になるが計算時間が増加する
  - CFG Scale：プロンプトの指示にどれだけ忠実に従うかの度合い（通常7前後）
- **VAE Decode**
  KSamplerが計算したAIの計算空間（潜在空間・Latent Space）のデータを、人間が視認できるピクセル画像（RGB）に変換する

## 2. 実装上の「詰まりポイント」と対策

初期段階で発生しやすい出力結果の違和感とその根本原因、対策である。

- **生成した人物の肌がツルツルになる（プラスチック感）**
  - **原因**：SD1.5の標準モデル（ベースモデル）は実写表現のデフォルト能力が低く、テクスチャがのっぺりしやすい
  - **対策**：プロンプトで質感（photorealistic, detailed skin, pores等）を強調するか、実写特化の派生モデルを利用する
- **イラストが不気味になる（不気味の谷現象）**
  - **原因**：標準モデルは実写とイラストの学習データが混在しているため、2Dの線画に生々しい3D的な影や質感が付与されやすい
  - **対策**：ネガティブプロンプトで3D要素（3d, realistic, cgi）を排除する。根本的にはアニメ・イラスト特化型モデル（Anything V5等）を導入することで解消される

## 3. API Format JSONの書き出しと構造理解

ComfyUIの最大の強みは、GUIで構築したワークフローをJSON形式のAPIペイロードとして書き出し、外部プログラム（Pythonやメッセージングボット等）からヘッドレスで実行できる点にある。

### 3.1 通常の保存（Save）とAPI用保存（Export API）の違い
プログラムから操作するためには、実行プロセスのみを抽出した「API Format JSON」が不可欠である。

| 比較項目 | 通常の保存 (Save) | API用保存 (Export API) |
| --- | --- | --- |
| **主な用途** | GUIへのワークフロー再読込 | Python等のプログラムからのHTTP POST実行 |
| **データ構造** | ノードの位置、色、UIの描画情報を含む | 実行に必要なノードの接続順（ルーティング）とパラメータ値のみ |
| **再利用性** | GUIで開き直す際に必須 | プログラム内で特定の値を動的に書き換える（変数化する）のに最適 |

### 3.2 API Format書き出しの有効化と手順
デフォルト状態ではAPI書き出しボタンが非表示の場合があるため、以下の手順で有効化して出力する。
1. 画面右側の設定メニュー（歯車アイコン）をクリックする
2. 設定一覧から 「Enable Dev mode Options」 を探し、チェックを入れる
3. 設定ウィンドウを閉じる
4. メニューパネルに 「Save (API Format)」 または 「エクスポート (API)」 ボタンが出現するのでクリックする
5. 任意の名前（例：`workflow_api.json`）でファイルを保存する

### 3.3 プログラム連携を見据えたJSONの構造確認
保存したJSONファイルをVS Code等のエディタで開き、データ構造を確認する。このJSONはノードIDをキーとした辞書型（連想配列）となっている。
以下はプロンプトを司る `CLIPTextEncode` ノードと、シード値を管理する `KSampler` ノードの抜粋例である。

```json
{
  "3": {
    "inputs": {
      "seed": 156680208700286,
      "steps": 20,
      "cfg": 8,
      "sampler_name": "euler",
      "scheduler": "normal",
      "denoise": 1,
      "model": [ "4", 0 ],
      "positive": [ "6", 0 ],
      "negative": [ "7", 0 ],
      "latent_image": [ "5", 0 ]
    },
    "class_type": "KSampler"
  },
  "6": {
    "inputs": {
      "text": "1girl, looking at viewer, highly detailed, masterpiece",
      "clip": [ "4", 1 ]
    },
    "class_type": "CLIPTextEncode"
  }
}
```

- **書き換えのポイント（プロンプト）**：上記JSONの `"6"` 番ノード内にある `inputs` -> `text` フィールドがポジティブプロンプトである。Python側で JSONを読み込んだ後、 `workflow["6"]["inputs"]["text"] = "新しいプロンプト"` のように書き換える
- **書き換えのポイント（シード値）**： `"3"` 番ノードの `inputs` -> `seed` をランダムな整数（例：`random.randint(1, 10**15)`）で上書きすることで、実行ごとに異なる画像を生成できる

## 4. 次のアクション

今回書き出した「API Format JSON」を利用し、PythonスクリプトからComfyUIのAPIエンドポイントへジョブを送信し、生成結果を受け取る基礎コードの実装（Phase 2 Step 3）へ進む。

# 【ComfyUI入門】自動化アーキテクチャの基盤：GUI基本操作とAPI JSONエクスポート手法

## 理由・背景：本記事のロードマップにおける位置づけ

筆者が取り組む「画像生成AI開発ロードマップ」の第1段階（Phase 1）に該当する実践記事である。
自動化システムを安定稼働させるためには、まず基盤となる単体ツールの入出力仕様を理解する必要がある。
アーキテクチャの全体設計や技術選定の背景については、前回のロードマップ記事を参照してほしい。

参考：[AI動画生成環境を構築するロードマップ](https://qiita.com/YukiF0511/items/92ba44b379241de11a96)

## サマリー（前提条件と5W1H）

- **実行環境（When/Where）**: クラウド環境（Paperspace等）およびローカルマシンのGPU環境
- **対象読者（Who）**: 画像生成システムをスクラッチ構築し、将来的に自動化したいエンジニア
- **最終的な解決状態（What）**: SD1.5モデルでの画像生成手法とAPI用JSONの出力方法を確立すること
- **アプローチの選定理由（Why）**: GUIでの視覚的理解を土台にし、Python経由のヘッドレス実行（画面なしバックグラウンド処理）へ円滑に移行するため
- **ステップの概要（How）**: ノードベースのデータフローを理解し、ワークフローをJSON化して構造を解析する

## 結論と成果物：API Format JSONの出力

本記事の最終成果物は、ComfyUIで構築した生成パイプラインの設定値群である「API Format JSON」である。
エンジニアはこのJSONファイルを抽出・編集し、外部プログラムからプロンプトを動的に操作できる。
以下は後述する手順で抽出するJSONのコア部分である。

```json
{
  "3": {
    "inputs": {
      "seed": 156680208700286,
      "text": "1girl, looking at viewer"
    },
    "class_type": "KSampler"
  }
}
```

## 具体的な実装・手順

ComfyUIはノード（処理ブロック）をワイヤーで結び、パイプラインを構築するアーキテクチャを採用している。
これは工場のベルトコンベアに似ており、上流の材料（テキスト）が各工程を経て下流で製品（画像）に加工される。

### 1. GUIの基本操作

ユーザーはキャンバス上で以下のマウス操作を実行する。

- **視点移動（パン）**: キャンバス空白部を左クリックしたままドラッグ、またはスペースキーと左ドラッグの併用
- **ズーム**: マウスホイールの回転
- **ノードの移動**: ノード上部ヘッダを左ドラッグ
- **ワイヤーの接続**: ノードの丸い端子（ピン）を左ドラッグして別の端子へ接続
- **ワイヤーの解除**: 接続済みの端子上で右クリック

### 2. 画像生成（Text to Image）のデータフロー

画像生成は「テキスト → ベクトルデータ → 潜在空間での微調整 → ピクセル画像」の順で推論が進行する。
各ノードが実行する処理の役割は以下の通りである。

- **Load Checkpoint**
  システムが画像生成の根幹となる学習済みAIモデルをメモリにロードする
  ※本記事ではリソース消費が少ないSD1.5（Stable Diffusion 1.5）を使用する
- **CLIP Text Encode**
  システムが人間の自然言語をAIが処理可能な数値配列（ベクトルデータ）へ変換する
  - ポジティブプロンプト: ユーザーが要求する描画対象（主題、背景、画質など）
  - ネガティブプロンプト: ユーザーが排除する要素（低画質、構造の崩れなど）
- **KSampler（Kサンプラー）**
  推論エンジンの心臓部であり、システムがノイズ空間から画像を徐々に削り出す（デノイズ）
  - Steps: ノイズ除去の反復回数（通常20〜30、多いほど高精細だが計算負荷が増加）
  - CFG Scale: AIがプロンプトの指示に従う強さ（通常7前後）
- **VAE Decode**
  システムが潜在空間（Latent Space）での計算結果を人間が視認できるRGB画像へ変換する

### 3. API Format JSONのアクセス有効化と書き出し

ComfyUIの最大の強みは、構築したワークフローをJSON形式でエクスポートできるエコシステムにある。
外部プログラムからヘッドレス実行を行う場合、このJSONファイルが必須となる。

通常の保存とAPI用保存の違いを以下に比較する。

| 比較項目 | 通常の保存 (Save) | API用保存 (Export API) |
| :--- | :--- | :--- |
| **主な用途** | ユーザーによるGUIへの再読込 | システムによるPython等からのHTTPリクエスト実行 |
| **データ構造** | ノードの位置や色などのUIレイアウト情報を含む | 実行に必要なルーティングと入力パラメータ値のみを保持 |
| **再利用性** | ブラウザで作業を再開する際に必須となる | 外部から特定変数を動的に注入・変更する用途に最適 |

ユーザーは以下の手順でAPIエクスポート機能を有効化する。

1. 画面右側の設定メニュー（歯車アイコン）をクリックする
2. 設定一覧から「Enable Dev mode Options」にチェックを入れる
3. 設定ウィンドウを閉じる
4. メニューパネルに出現した「Save (API Format)」ボタンをクリックする
5. 任意のファイルネーム（例：`workflow_api.json`）で保存を実行する

### 4. JSONデータ構造の解析と変数化

保存したJSONをエディタで確認すると、ノードIDをキーとした辞書型（連想配列）であることがわかる。
下記はプロンプト（CLIPTextEncode）とシード値（KSampler）の抜粋である。

```json
{
  "3": {
    "inputs": {
      "seed": 156680208700286,
      "steps": 20,
      "cfg": 8,
      "sampler_name": "euler",
      "scheduler": "normal",
      "denoise": 1,
      "model": [ "4", 0 ],
      "positive": [ "6", 0 ],
      "negative": [ "7", 0 ],
      "latent_image": [ "5", 0 ]
    },
    "class_type": "KSampler"
  },
  "6": {
    "inputs": {
      "text": "1girl, looking at viewer, highly detailed, masterpiece",
      "clip": [ "4", 1 ]
    },
    "class_type": "CLIPTextEncode"
  }
}
```

エンジニアは自動化スクリプトにて、これらのノードパラメータを動的に上書きする。

- **プロンプトの変更**: `workflow["6"]["inputs"]["text"] = "新しいプロンプト"` のように値を置換する
- **シード値の変更**: `"3"` 番ノードの `seed` を任意の整数で上書きする

## 落とし穴と対策（詰まりポイント）

読者が画像生成プロセスで遭遇しやすいペインポイント（問題）と、解決策を以下に定義する。

- **生成人物の肌がプラスチックのような不自然な質感になる**
  - **原因**: ベースモデル（SD1.5）は実写描写の能力が低く、テクスチャが単調になりやすい
  - **対策**: プロンプトで質感（photorealistic, detailed skin等）を強調するか、実写特化モデルを導入する
- **イラストの影や立体感が不気味になる（不気味の谷現象）**
  - **原因**: 2Dと3Dの学習情報が混在し、アニメ調の線画に生々しい影が付与されるため
  - **対策**: ネガティブプロンプトで3D属性（3d, realistic, cgi）を明示的に排除する

## ネクストアクション

本記事で出力した「API Format JSON」を利用し、外部スクリプトからの自動実行フェーズへ移行する。
ユーザーはPythonを用いて、ComfyUIのAPIエンドポイントへHTTP POSTリクエストを送信するプログラムを実装・検証する。
